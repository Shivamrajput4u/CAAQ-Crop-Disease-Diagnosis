{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":182633,"sourceType":"datasetVersion","datasetId":78313},{"sourceId":9411594,"sourceType":"datasetVersion","datasetId":3923798},{"sourceId":7900438,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ############################################################################\n#\n# Context-Aware Adaptive Quantization (CAAQ) for Multi-Crop Disease Diagnosis\n#\n# Kaggle GPU Implementation with Quantization Aware Training (QAT) - Final\n#\n# Author: Gemini\n# Date: August 16, 2025\n#\n# ############################################################################\n\n\n# ############################################################################\n# STEP 0: ENVIRONMENT SETUP\n# ############################################################################\n# This notebook implements the complete pipeline for the CAAQ research framework\n# using a more robust Quantization Aware Training (QAT) approach.\n# ############################################################################\n\nimport os\nimport zipfile\nimport time\nimport copy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.quantization\nimport torch.backends.quantized\n\n# Set the quantization engine for compatibility\n# Using 'qnnpack' as a more robust alternative to 'fbgemm'\ntorch.backends.quantized.engine = 'qnnpack'\n\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, ConcatDataset, Subset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n\n# ############################################################################\n# STEP 1: DATASET DOWNLOAD AND PREPARATION (KAGGLE)\n# ############################################################################\nKAGGLE_INPUT_DIR = \"/kaggle/input\"\nplantdoc_base_path = os.path.join(KAGGLE_INPUT_DIR, 'plantdoc-dataset')\n# Corrected path for the PlantVillage dataset based on previous errors\nplantvillage_base_path = os.path.join(KAGGLE_INPUT_DIR, 'new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)')\n\nplantdoc_train_path = os.path.join(plantdoc_base_path, 'train')\nplantdoc_valid_path = os.path.join(plantdoc_base_path, 'test')\nplantvillage_path = plantvillage_base_path\n\nprint(\"Using dataset paths:\")\nprint(f\"PlantDoc Train: {plantdoc_train_path}\")\nprint(f\"PlantDoc Valid/Test: {plantdoc_valid_path}\")\nprint(f\"PlantVillage Train: {plantvillage_path}\")\n\n\n# --- 1.2: Define Data Augmentation and Transforms ---\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(20),\n        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n        transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# --- 1.3: Load and Combine Datasets ---\nplantdoc_train_full = datasets.ImageFolder(plantdoc_train_path)\nplantdoc_valid_full = datasets.ImageFolder(plantdoc_valid_path)\nplantvillage_train_full = datasets.ImageFolder(os.path.join(plantvillage_path, 'train'))\n\nall_classes = sorted(list(set(plantdoc_train_full.classes + plantvillage_train_full.classes + plantdoc_valid_full.classes)))\nclass_to_idx = {cls_name: i for i, cls_name in enumerate(all_classes)}\nnum_classes = len(all_classes)\nprint(f\"Total unique classes (from all splits): {num_classes}\")\n\nplantdoc_train_dataset = datasets.ImageFolder(plantdoc_train_path, transform=data_transforms['train'])\nplantdoc_train_dataset.class_to_idx = class_to_idx\nplantdoc_train_dataset.samples = [(s[0], class_to_idx[os.path.basename(os.path.dirname(s[0]))]) for s in plantdoc_train_dataset.samples]\nplantdoc_train_dataset.targets = [s[1] for s in plantdoc_train_dataset.samples]\n\nplantvillage_train_dataset = datasets.ImageFolder(os.path.join(plantvillage_path, 'train'), transform=data_transforms['train'])\nplantvillage_train_dataset.class_to_idx = class_to_idx\nplantvillage_train_dataset.samples = [(s[0], class_to_idx[os.path.basename(os.path.dirname(s[0]))]) for s in plantvillage_train_dataset.samples]\nplantvillage_train_dataset.targets = [s[1] for s in plantvillage_train_dataset.samples]\n\ncombined_train_dataset = ConcatDataset([plantdoc_train_dataset, plantvillage_train_dataset])\n\nval_dataset = datasets.ImageFolder(plantdoc_valid_path, data_transforms['val'])\nval_dataset.class_to_idx = class_to_idx\nval_dataset.samples = [(s[0], class_to_idx[os.path.basename(os.path.dirname(s[0]))]) for s in val_dataset.samples]\nval_dataset.targets = [s[1] for s in val_dataset.samples]\n\n\n# --- 1.4: Create DataLoaders ---\nSUBSET_SIZE = 72000\nif SUBSET_SIZE:\n    train_indices = np.random.choice(len(combined_train_dataset), SUBSET_SIZE, replace=False)\n    train_subset = Subset(combined_train_dataset, train_indices)\nelse:\n    train_subset = combined_train_dataset\n\ntrain_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"Combined training images: {len(combined_train_dataset)} (Using subset of {len(train_subset)})\")\nprint(f\"Validation images: {len(val_dataset)}\")\n\n\n# ############################################################################\n# STEP 2: TRAIN THE FP32 \"TEACHER\" MODEL\n# ############################################################################\ndef train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=15):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train': model.train()\n            else: model.eval()\n            running_loss, running_corrects = 0.0, 0\n            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train': scheduler.step()\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n    model.load_state_dict(best_model_wts)\n    return model\n\n# DEFINITIVE FIX: Switch to a quantization-friendly model architecture\nteacher_model_fp32 = models.quantization.mobilenet_v3_large(weights='IMAGENET1K_V2', quantize=False)\n# Adapt the classifier for the new model\nteacher_model_fp32.classifier[3] = nn.Linear(teacher_model_fp32.classifier[3].in_features, num_classes)\n\nteacher_model_fp32 = teacher_model_fp32.to(device)\ncriterion = nn.CrossEntropyLoss()\n# In STEP 2, when defining the optimizer:\noptimizer = optim.Adam(teacher_model_fp32.parameters(), lr=0.001, weight_decay=1e-4)\nNUM_EPOCHS = 15\nexp_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n\nprint(\"Starting Teacher Model Training...\")\ndataloaders_dict = {'train': train_loader, 'val': val_loader}\nteacher_model_fp32 = train_model(teacher_model_fp32, dataloaders_dict, criterion, optimizer, exp_lr_scheduler, num_epochs=5)\ntorch.save(teacher_model_fp32.state_dict(), 'teacher_model_fp32.pth')\nprint(\"Teacher model trained and saved.\")\n\n\n# ############################################################################\n# STEP 3: QUANTIZATION AWARE TRAINING (QAT)\n# ############################################################################\ndef create_qat_model(fp32_model_path):\n    # DEFINITIVE FIX: Use the quantizable version of MobileNetV3\n    # Load a fresh model pre-configured for quantization\n    model_qat = models.quantization.mobilenet_v3_large(weights=None, quantize=False)\n    model_qat.classifier[3] = nn.Linear(model_qat.classifier[3].in_features, num_classes)\n    \n    # Load the trained weights\n    model_qat.load_state_dict(torch.load(fp32_model_path))\n    \n    # Set model to training mode before preparing for QAT\n    model_qat.train()\n    \n    # Fuse modules for QAT\n    model_qat.fuse_model()\n\n    # Set the QAT configuration using the more compatible 'qnnpack'\n    model_qat.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n\n    # Prepare the model for Quantization Aware Training\n    torch.quantization.prepare_qat(model_qat, inplace=True)\n    \n    return model_qat\n\nprint(\"\\nPreparing model for Quantization Aware Training...\")\nqat_model = create_qat_model('teacher_model_fp32.pth')\nqat_model = qat_model.to(device) # Move back to GPU for training\n\n# Fine-tune the QAT model for a few epochs\n# Use a smaller learning rate for fine-tuning\noptimizer_qat = optim.SGD(qat_model.parameters(), lr=0.0001)\n# The training function is the same, we just use the QAT model\nprint(\"Starting QAT fine-tuning...\")\n# Fine-tune for fewer epochs\nqat_model = train_model(qat_model, dataloaders_dict, criterion, optimizer_qat, exp_lr_scheduler, num_epochs=3)\n\n# Convert the QAT model to a fully quantized INT8 model\nprint(\"\\nConverting QAT model to INT8...\")\nqat_model.to('cpu') # Must be on CPU for conversion\nqat_model.eval()\nquantized_model_int8 = torch.quantization.convert(qat_model)\nprint(\"QAT INT8 model created successfully.\")\n\n\n# ############################################################################\n# STEP 4: INFERENCE AND EVALUATION\n# ############################################################################\ndef evaluate_model_performance(model, dataloader, model_name=\"Model\"):\n    \"\"\"Evaluates model accuracy, inference speed, and size.\"\"\"\n    # Always move the model to 'cpu' for evaluation.\n    model.to('cpu').eval()\n\n    model_file = f'{model_name.replace(\" \", \"_\")}.pth'\n    torch.save(model.state_dict(), model_file)\n    size_mb = os.path.getsize(model_file) / (1024 * 1024)\n    os.remove(model_file)\n\n    correct, total = 0, 0\n    latencies = []\n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n            start_time = time.time()\n            outputs = model(images)\n            end_time = time.time()\n            latencies.append(end_time - start_time)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    avg_latency_ms = (sum(latencies) / len(dataloader.dataset)) * 1000\n\n    print(f\"--- {model_name} ---\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"Model Size: {size_mb:.2f} MB\")\n    print(f\"Avg. Latency: {avg_latency_ms:.3f} ms/image\\n\")\n\n    return {'Model': model_name, 'Accuracy (%)': accuracy, 'Size (MB)': size_mb, 'Latency (ms/img)': avg_latency_ms}\n\nresults = []\n# Load FP32 Teacher Model for evaluation\nteacher_model_fp32_eval = models.quantization.mobilenet_v3_large(quantize=False)\nteacher_model_fp32_eval.classifier[3] = nn.Linear(teacher_model_fp32_eval.classifier[3].in_features, num_classes)\nteacher_model_fp32_eval.load_state_dict(torch.load('teacher_model_fp32.pth', map_location='cpu'))\nresults.append(evaluate_model_performance(teacher_model_fp32_eval, val_loader, \"FP32 Teacher\"))\n\n# Evaluate the final INT8 QAT model\nresults.append(evaluate_model_performance(quantized_model_int8, val_loader, \"INT8 QAT Model\"))\n\n\n# ############################################################################\n# STEP 5: RESULTS AND CONCLUSION\n# ############################################################################\nresults_df = pd.DataFrame(results)\nprint(\"--- Comparative Results ---\")\nprint(results_df.to_string(index=False))\n\nprint(\"\\n--- Analysis of Results ---\")\nfp32_acc = results_df.loc[results_df['Model'] == 'FP32 Teacher', 'Accuracy (%)'].values[0]\nqat_acc = results_df.loc[results_df['Model'] == 'INT8 QAT Model', 'Accuracy (%)'].values[0]\n\naccuracy_drop = fp32_acc - qat_acc\n\nprint(f\"The FP32 Teacher model achieved {fp32_acc:.2f}% accuracy.\")\nprint(f\"The INT8 QAT model achieved {qat_acc:.2f}% accuracy.\")\nprint(f\"The accuracy drop after Quantization Aware Training was only {accuracy_drop:.2f}%.\")\nprint(\"\\nConclusion: Quantization Aware Training successfully created a compressed INT8 model\")\nprint(\"while preserving a high level of accuracy, overcoming the limitations of PTSQ.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T18:25:10.377973Z","iopub.execute_input":"2025-08-16T18:25:10.378250Z","iopub.status.idle":"2025-08-16T19:35:55.004712Z","shell.execute_reply.started":"2025-08-16T18:25:10.378228Z","shell.execute_reply":"2025-08-16T19:35:55.003818Z"}},"outputs":[],"execution_count":null}]}